{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Classes\n",
    "Connection classes are a place to specify all the connections that a task intends to make with the rest of a larger pipeline. The class has the following connection types:\n",
    "* InitInput - arguments that will be passed to init, dimensionless, normally things like schemas\n",
    "* InitOutputs - Values that the activator system will retrive once init is done, normally schemas post init\n",
    "* Inputs - Input dataset types that are used to constrain an execution graph (i.e. the data that will be proccessed.\n",
    "* PrerequisiteInputs - Datasets produced outside the pipeline that are required, these arguments don't constrain what data will be processed. This will be things like reference catalogs or bright object masks\n",
    "* Output - A specification of what dataset types this task will produce\n",
    "\n",
    "Each of these connection types are declared with arguments\n",
    "\n",
    "* name - The dataset type name\n",
    "* storageClass - The storage type of this object in the datastore\n",
    "* deferLoad - Optional, default False. Tells the butler to return a defer object the task will load from\n",
    "* multiple - Optional, default False. Indicates this field will expect a list of python objects\n",
    "* checkFunction - Optional, default None. Function that allows verification of the quantum the activator will produce\n",
    "\n",
    "Additionally Inputs, PrerequisiteInputs, and Outputs take\n",
    "\n",
    "* dimensions - The set of dimensions that specify a particular connection, i.e. visit, detector, instrument\n",
    "\n",
    "Connection classes must also be declared with a dimensions attribute, which specifies the fundimental unit this task will process, i.e. tract, patch\n",
    "\n",
    "The name fields on connections can be specified as a python format string, with a name in each of the template fields, i.e. \"{coaddType}Coadd_meas\". If any name contains a template, then the connection class must be declared with a defaultTemplate attribute that is a dictionary specifying what the default value will be.\n",
    "\n",
    "#### Instantiation\n",
    "Connection classes are instantiated with a config object (discussed below) that allow overriding connection names, as well as controling if a partiuclar connection will be used when building a quantum graph (an execution flow). The set of each type of connection can be found under:\n",
    " * self.initInputs\n",
    " * self.initOutputs\n",
    " * self.inputs\n",
    " * self.prerequisiteInputs\n",
    " * self.outputs\n",
    " \n",
    " These names must not be otherwise used in the init method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.pipe.base import PipelineTaskConfig, PipelineTaskConnections, PipelineTask\n",
    "import lsst.pex.config as pexConfig\n",
    "\n",
    "class ExamplePipelineTaskConnections(PipelineTaskConnections):\n",
    "    inputSchema = InitInput(name=\"dummy_schema\",\n",
    "                            storageClass=\"SourceCatalog\")\n",
    "    outputSchema = InitOutput(name=\"example_dummy_schema\",\n",
    "                              storageClass=\"SourceCatalog\")\n",
    "    exposure = Input(name=\"calexp\", storageClass=\"ExposureF\",\n",
    "                     dimensions=(\"instrument\", \"visit\", \"detector\"))\n",
    "    brightStarMask = PrerequisiteInput(name=\"bright_star_mask\",\n",
    "                                       storageClass=\"StarMasks\",\n",
    "                                       dimensions=(\"tract\", \"patch\"))\n",
    "    resultExp = Output(name=\"{outputTemp}Exposure\", storageClass=\"ExposureF\")\n",
    "    resultCat = Output(name=\"{outputTemp}meas_cat\", storageClass=\"SourceCatalog\",\n",
    "                       dimensions=(\"instrument\", \"visit\", \"detector\"))\n",
    "    \n",
    "    dimensions = (\"instrument\", \"visit\", \"detector\")\n",
    "    defaultTemplates = {\"outputTemp\": \"Example\"}\n",
    "    \n",
    "    def __init__(self, *, config=None):\n",
    "        super().__init__(config=config)\n",
    "        if not config.doThings:\n",
    "            self.prerequisiteInputs -= set((\"brightStarMask\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipelineTaskConfig\n",
    "These are almost standard configuration classes, but they must be declared with a pipelineConnections keyword in addition to the `PipelineTaskConfig` base class. This adds all the configurable elements of the specified PipelineConnectionsClass to the config class under a field called connections. Right now this is only the name fields, and the names of the templates in format strings if any are present.\n",
    "\n",
    "These fields are then set just like any normal configuration. If templates are present, assigning to them will have the effect of formating all the format strings when the config is later used in constructing a connection class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePipelineTaskConfig(PipelineTaskConfig, pipelineConnections=ExamplePipelineTaskConnections):\n",
    "    doThings = pexConfig.Field(dtype=bool, default=True, doc=\"Example field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipelineTask\n",
    "This is the task that will be called by an activator to process a quantum of data. \n",
    "\n",
    "#### Init\n",
    "`InitInput` connections specified in the connections class will be passed into the `__init__` method inside an `initInputs` argument. It is expected that any declared `InitOutput`s will be assigned to an instance variable with the same variable name used in the connection class. The activator will look for this to write as an output.\n",
    "\n",
    "#### Execution\n",
    "Execution happens inside the runQuantum class. This is passed a unit of work to process as defined by the dimensions on the connection class, i.e. a tract, patch. This function is responsable for loading the provided `DatasetRef`s, executing the tasks `Run` method, and saving the output. \n",
    "\n",
    "`runQuantum` is provided a `ButlerQuantumContext` object to do the getting and putting. This is like a regular butler, but it only allows loading datasets defined in the connections class. The other arguments to runQuantum are `Struct`s that map the attribut names used in the connection class to `DatasetRef`s.\n",
    "\n",
    "The butlerQC object, can take one of these `Structs`, a single `DatasetRef`, or a `list` of `DataRefs`s. If the struct is provided to get, it will load everything into a dictionary keyed by the attribute name in the connection class. If a single `DatasetRef` is provided a single output will be returned. If a `list` of `DataRefs`s is given a dictionary of `DataIds` to object will be returned. The put method behaves similarly. If a struct is given it will look up the corresponding fields in the \"values\" struct and save them. If a single `DatasetRef` is given, the \"value\" argument of put must be a single value. If a dict is given it must be a mapping of `DataId` to object, and the \"value\" argument of the put will be the dataset type name (the name field on a connection, the same thing that can be set on the config class).\n",
    "\n",
    "The `run` method should accept keywords corresponding to the `Input` and `PrerequisiteInput` attribute names defined in the connection class. It should return a `Struct` corresponding to the attribute names of the `Output` connections on the connection class. This is only a requirement if the default `runQuantum` is used. It is a requirement for run to return this sort of `Struct` if `butlerQC.put` is to be called directly with the `outputRefs` `Struct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePipelineTask(PipelineTask):\n",
    "    def __init__(self, config, *args, initInputs=None, **kwargs):\n",
    "        super().__init__(*args, config=config, **kwargs)\n",
    "        self.outputSchema = addToInputSchema(initInputs['inputSchema'])\n",
    "    \n",
    "    # This is the method from the base class, it is redefined here as an example only\n",
    "    def runQuantum(self, butlerQC, inputRefs, outputRefs):\n",
    "        inputs = butlerQC.get(inputRefs)\n",
    "        outputs = self.run(**inputs)\n",
    "        butlerQC.put(outputs, outputRefs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: inputSchema     Value: dummy_schema\n",
      "Field: outputSchema    Value: example_dummy_schema\n",
      "Field: exposure        Value: calexp\n",
      "Field: brightStarMask  Value: bright_star_mask\n",
      "Field: resultExp       Value: {outputTemp}Exposure\n",
      "Field: resultCat       Value: {outputTemp}meas_cat\n",
      "Field: outputTemp      Value: Example\n"
     ]
    }
   ],
   "source": [
    "# Create a config object, alter some properties\n",
    "config = ExamplePipelineTaskConfig()\n",
    "_=[print(f\"Field: {k: <15} Value: {v}\") for k, v in config.connections.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name for the inputScheam\n",
    "config.connections.inputSchema = 'previous_step_schema'\n",
    "# format the shared template name 'outputTemp'\n",
    "config.connections.outputTemp = 'bgMasked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_step_schema\n",
      "bgMaskedExposure\n",
      "bgMaskedmeas_cat\n"
     ]
    }
   ],
   "source": [
    "# Create a connections class to see what happened (This is all done by the system but is done here to demo)\n",
    "connections = ExamplePipelineTaskConnections(config=config)\n",
    "# Print the names of the modified fields\n",
    "print(connections.inputSchema.name)\n",
    "print(connections.resultExp.name)\n",
    "print(connections.resultCat.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
